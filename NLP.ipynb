{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PZ24Igf2CWO"
      },
      "source": [
        "\n",
        "\n",
        "> Transcribed text from the video- for example an transcribed text has been added to this cell in string format\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Gt0dsv2JiI"
      },
      "source": [
        "transcript = \"I'm a Stella when you travel abroad, it's always a good idea to pick up some common phrases in the local language. Even though english is commonly used in most countries around the world. Now, not everybody feels comfortable speaking to a random stranger on industry. So if you are able to break the eyes by using their own language, you will definitely be appreciated and make a good first impression. Here are the top 10 phrases to help you out in chinese ni hao and simple greeting like, hi, hello, hey, die jen, I'll see you again. Goodbye. She, she always an important thing to know. Thank you ship. She literally, this means no plight what people will hear however, is you're welcome, wore people down a very important phrase in any language I don't understand. So since um, ah, here's one that might get used quite a lot. What is this site now lee, if you are lost, are trying to find a specific place, then you'll need this. Just put the place name at the beginning, followed by side. Na lee and you've got where is washout. True fun. It's hard to get around on an empty stomach. So this way will help you find something delicious. I want to eat the wash out here. Since you might not always see a price tag. Being able to ask how much it's good to know. Well y'all sugar, there might be a lot of times when you're unsure what something is called, but you know, you want it in that case pointing to it and saying, I want this one can help you get what you need with these phrases. You'll be well on your way to traveling around and meeting new people in chinese speaking countries to learn more vocabulary and useful phrases come over to our basic mandarin on addicts. Also follow us on facebook and twitter to get updated about mandarin\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukyy9rNr3PS8"
      },
      "source": [
        "Importing Libraries....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZhWDU4ZwNUk",
        "outputId": "20668498-6e7a-4e53-bd06-3a37c383d409"
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import statistics\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd     #(version 1.0.0)\n",
        "#import plotly           #(version 4.5.0)\n",
        "#import plotly.express as px\n",
        "#import plotly.io as pio\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz58r_3j4JLQ"
      },
      "source": [
        "**Function for sentiment analysis and valence arousal**\n",
        "\n",
        "Note: the path here has to be the directory where the EnglishShortened.csv will be, if it do not detect the path automatically please put the directory location manually in path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Y14Xo7Mt6v",
        "outputId": "5804208f-5936-486e-fe6f-2db592101049"
      },
      "source": [
        "\n",
        "path = '../'\n",
        "lmtzr = WordNetLemmatizer()\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "#anew = \"../lib/vad-nrc.csv\"\n",
        "anew = path + \"EnglishShortened.csv\"\n",
        "avg_V = 5.06    # average V from ANEW dict\n",
        "avg_A = 4.21\n",
        "avg_D = 5.18\n",
        "\n",
        "# performs sentiment analysis on inputFile using the ANEW database, outputting results to a new CSV file in outputDir\n",
        "def analyzefile(line, mode):\n",
        "    s = tokenize.word_tokenize(line.lower())\n",
        "  \n",
        "    all_words = []\n",
        "    found_words = []\n",
        "    total_words = 0\n",
        "    v_list = []  # holds valence scores\n",
        "    a_list = []  # holds arousal scores\n",
        "    d_list = []  # holds dominance scores\n",
        "\n",
        "    words = nltk.pos_tag(s)\n",
        "    for index, p in enumerate(words):\n",
        "        w = p[0]\n",
        "        pos = p[1]\n",
        "        if w in stops or not w.isalpha():\n",
        "            continue\n",
        "        j = index-1\n",
        "        neg = False\n",
        "        while j >= 0 and j >= index-3:\n",
        "            \n",
        "            if words[j][0] == 'not' or words[j][0] == 'no' or words[j][0] == 'n\\'t':\n",
        "                neg = True\n",
        "                break\n",
        "            j -= 1\n",
        "\n",
        "\n",
        "        if pos[0] == 'N' or pos[0] == 'V':\n",
        "            lemma = lmtzr.lemmatize(w, pos=pos[0].lower())\n",
        "        else:\n",
        "            lemma = w\n",
        "\n",
        "        all_words.append(lemma)\n",
        "\n",
        "        with open(anew) as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                if row['Word'].casefold() == lemma.casefold():\n",
        "                    if neg:\n",
        "                        found_words.append(\"neg-\"+lemma)\n",
        "                    else:\n",
        "                        found_words.append(lemma)\n",
        "                    v = float(row['valence'])\n",
        "                    a = float(row['arousal'])\n",
        "                    d = float(row['dominance'])\n",
        "\n",
        "                    if neg:\n",
        "                        \n",
        "                        v = 5 - (v - 5)\n",
        "                        a = 5 - (a - 5)\n",
        "                        d = 5 - (d - 5)\n",
        "\n",
        "                    v_list.append(v)\n",
        "                    a_list.append(a)\n",
        "                    d_list.append(d)\n",
        "\n",
        "    if len(found_words) == 0:\n",
        "        Valence = 0\n",
        "        Arousal = 0\n",
        "        Dominance = 0\n",
        "    \n",
        "    else:\n",
        "        if mode == 'median':\n",
        "            sentiment = statistics.median(v_list)\n",
        "            arousal = statistics.median(a_list)\n",
        "            dominance = statistics.median(d_list)\n",
        "        elif mode == 'mean':\n",
        "            sentiment = statistics.mean(v_list)\n",
        "            arousal = statistics.mean(a_list)\n",
        "            dominance = statistics.mean(d_list)\n",
        "        elif mode == 'mika':\n",
        "            # calculate valence\n",
        "            if statistics.mean(v_list) < avg_V:\n",
        "                sentiment = max(v_list) - avg_V\n",
        "            elif max(v_list) < avg_V:\n",
        "                sentiment = avg_V - min(v_list)\n",
        "            else:\n",
        "                sentiment = max(v_list) - min(v_list)\n",
        "            # calculate arousal\n",
        "            if statistics.mean(a_list) < avg_A:\n",
        "                arousal = max(a_list) - avg_A\n",
        "            elif max(a_list) < avg_A:\n",
        "                arousal = avg_A - min(a_list)\n",
        "            else:\n",
        "                arousal = max(a_list) - min(a_list)\n",
        "            # calculate dominance\n",
        "            if statistics.mean(d_list) < avg_D:\n",
        "                dominance = max(d_list) - avg_D\n",
        "            elif max(d_list) < avg_D:\n",
        "                dominance = avg_D - min(a_list)\n",
        "            else:\n",
        "                dominance = max(d_list) - min(d_list)\n",
        "        else:\n",
        "            raise Exception('Unknown mode')\n",
        "\n",
        "          # set sentiment label\n",
        "        label = 'neutral'\n",
        "        if sentiment > 6:\n",
        "            label = 'positive'\n",
        "        elif sentiment < 4:\n",
        "            label = 'negative'\n",
        "\n",
        "        Valence = sentiment\n",
        "        Arousal = arousal\n",
        "        Dominance = dominance\n",
        "        Valence = 0.25*(Valence - 1) - 1\n",
        "        Arousal = 0.25*(Arousal - 1) - 1\n",
        "        Dominance = 0.25*(Dominance - 1) - 1\n",
        "\n",
        "    ls_expr_intensity = [\n",
        "      \"Slightly\", \"Moderately\", \"Very\", \"Extremely\"\n",
        "      ]\n",
        "    ls_expr_name = [\n",
        "      \"pleased\", \"happy\", \"delighted\", \"excited\", \"astonished\", \n",
        "      \"aroused\", # first quarter\n",
        "\n",
        "      \"tensed\", \"alarmed\", \"afraid\", \"annoyed\", \"distressed\", \n",
        "      \"frustrated\", \"miserable\", # second quarter\n",
        "\n",
        "      \"sad\", \"gloomy\", \"depressed\", \"bored\", \"droopy\", \"tired\", \n",
        "      \"sleepy\", # third quarter\n",
        "\n",
        "      \"calm\", \"serene\", \"content\", \"satisfied\"  # fourth quarter\n",
        "  ]\n",
        "\n",
        "  # analyzing intensity\n",
        "    if Dominance < 0.05 and Valence < 0.01 and Valence > -0.01 and Arousal >-0.01 and Arousal <0.01:\n",
        "        expression_name = \"Neutral\"\n",
        "        expression_intensity = \"\"\n",
        "    else: \n",
        "        if Dominance < 0.225:\n",
        "            expression_intensity = ls_expr_intensity[0]\n",
        "        elif Dominance < 0.45:\n",
        "            expression_intensity = ls_expr_intensity[1]\n",
        "        elif Dominance < 0.705:\n",
        "            expression_intensity = ls_expr_intensity[2]\n",
        "        else:\n",
        "            expression_intensity = ls_expr_intensity[3]\n",
        "        if Valence == 0:\n",
        "            if Arousal >= 0:\n",
        "                theta = 90\n",
        "            else:\n",
        "                theta = 270\n",
        "        else:\n",
        "            theta = math.atan(Arousal / Valence)\n",
        "            theta = theta * (180 / math.pi)\n",
        "\n",
        "            if Valence < 0:\n",
        "                theta = 180 + theta\n",
        "            elif Arousal < 0:\n",
        "                theta = 360 + theta\n",
        "\n",
        "\n",
        "        if theta < 15 or theta > 354:\n",
        "            expression_name = ls_expr_name[0]\n",
        "        elif theta < 30:\n",
        "            expression_name = ls_expr_name[1]\n",
        "        elif theta < 45.5:\n",
        "            expression_name = ls_expr_name[2]\n",
        "        elif theta < 60:\n",
        "            expression_name = ls_expr_name[3]\n",
        "        elif theta < 75:\n",
        "            expression_name = ls_expr_name[4]\n",
        "        elif theta < 90:\n",
        "            expression_name = ls_expr_name[5]\n",
        "        elif theta < 105:\n",
        "            expression_name = ls_expr_name[6]\n",
        "        elif theta < 120:\n",
        "            expression_name = ls_expr_name[7]\n",
        "        elif theta < 135:\n",
        "            expression_name = ls_expr_name[8]\n",
        "        elif theta < 150:\n",
        "            expression_name = ls_expr_name[9]\n",
        "        elif theta < 165:\n",
        "            expression_name = ls_expr_name[10]\n",
        "        elif theta < 180:\n",
        "            expression_name = ls_expr_name[11]\n",
        "        elif theta < 195:\n",
        "            expression_name = ls_expr_name[12]\n",
        "        elif theta < 210:\n",
        "            expression_name = ls_expr_name[13]\n",
        "        elif theta < 225:\n",
        "            expression_name = ls_expr_name[14]\n",
        "        elif theta < 240:\n",
        "            expression_name = ls_expr_name[15]\n",
        "        elif theta < 255:\n",
        "            expression_name = ls_expr_name[16]\n",
        "        elif theta < 260:\n",
        "            expression_name = ls_expr_name[17]\n",
        "        elif theta < 275:\n",
        "            expression_name = ls_expr_name[18]\n",
        "        elif theta < 290:\n",
        "            expression_name = ls_expr_name[19]\n",
        "        elif theta < 305:\n",
        "            expression_name = ls_expr_name[20]\n",
        "        elif theta < 320:\n",
        "            expression_name = ls_expr_name[21]\n",
        "        elif theta < 335:\n",
        "            expression_name = ls_expr_name[22]\n",
        "        elif theta < 354:\n",
        "            expression_name = ls_expr_name[23]\n",
        "        else:\n",
        "            expression_name = \"Unknown\"\n",
        "            expression_intensity = \"\"\n",
        "\n",
        "  # TODO: return also variable output and not only string\n",
        "\n",
        "\n",
        "   # i += 1\n",
        "    return Valence, Arousal, Dominance, expression_intensity + \" \" + expression_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl9Ru-OB5C4x"
      },
      "source": [
        "**Tokenization into sentences and storing the output in terms of Valence, arousal and emotion tag**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjJy6LJNSRvV"
      },
      "source": [
        "a_list = nltk.tokenize.sent_tokenize(transcript)\n",
        "\n",
        "a_a = []\n",
        "v_a = []\n",
        "e_a = []\n",
        "\n",
        "for l in a_list:\n",
        "    valence, arousal, dom,emotion_name = analyzefile(l, 'mean')\n",
        "  \n",
        "    a_a.append(arousal)\n",
        "    v_a.append(valence) \n",
        "    e_a.append(emotion_name) \n",
        "\n",
        "df1 = pd.DataFrame(list(zip(v_a, a_a,e_a)), columns =['valence', 'arousal','emotion'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt1jzRN-Na5L"
      },
      "source": [
        "df1['sentence']= a_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF4SSHmSLNHI"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}